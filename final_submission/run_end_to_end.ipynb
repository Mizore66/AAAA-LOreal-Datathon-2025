{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a25b223b",
   "metadata": {},
   "source": [
    "# End-to-End Pipeline Runner\n",
    "This notebook sequentially executes the ingestion, full pipeline, and term decay analysis scripts located in `final_submission/src`.\n",
    "\n",
    "Order of execution:\n",
    "1. Verify environment & script presence\n",
    "2. Run data ingestion (`ingest_provided_data.py`)\n",
    "3. Run full pipeline (`full_pipeline.py`)\n",
    "4. Run term decay analysis (`term_decay_analysis.py`)\n",
    "5. Summarize produced artifacts\n",
    "6. (Optional) Parameterized execution template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fddb57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Set Working Directory & Verify Scripts\n",
    "import os, pathlib, sys, json, datetime\n",
    "from pprint import pprint\n",
    "\n",
    "PROJECT_ROOT = pathlib.Path.cwd()\n",
    "# Try to locate the final_submission/src directory relative to notebook location\n",
    "candidate = PROJECT_ROOT / 'final_submission' / 'src'\n",
    "if not candidate.is_dir():\n",
    "    # If notebook placed at project root, adjust\n",
    "    alt = pathlib.Path('final_submission/src').resolve()\n",
    "    if alt.is_dir():\n",
    "        candidate = alt\n",
    "\n",
    "SRC = candidate.resolve()\n",
    "os.chdir(SRC)\n",
    "print(f\"Working directory set to: {SRC}\")\n",
    "\n",
    "required_scripts = ['ingest_provided_data.py','full_pipeline.py','term_decay_analysis.py']\n",
    "missing = [s for s in required_scripts if not (SRC / s).is_file()]\n",
    "assert not missing, f\"Missing scripts: {missing}\"\n",
    "print(\"All required scripts present.\")\n",
    "\n",
    "print(\"Listing src contents (truncated):\")\n",
    "for p in list(SRC.iterdir())[:25]:\n",
    "    print('-', p.name)\n",
    "\n",
    "# Append src to sys.path if not present\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC))\n",
    "print(\"sys.path updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7c7a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Run Data Ingestion Script\n",
    "import subprocess, time, pathlib, os, json\n",
    "from pprint import pprint\n",
    "\n",
    "start = time.time()\n",
    "print(\"Running ingestion script: ingest_provided_data.py\")\n",
    "result = subprocess.run([sys.executable, 'ingest_provided_data.py'], capture_output=True, text=True)\n",
    "elapsed = time.time() - start\n",
    "print(f\"Ingestion completed in {elapsed:.2f}s with return code {result.returncode}\\n\")\n",
    "print(\"--- STDOUT (first 3000 chars) ---\")\n",
    "print(result.stdout[:3000])\n",
    "print(\"--- STDERR (first 1000 chars) ---\")\n",
    "print(result.stderr[:1000])\n",
    "\n",
    "# Show key data directories if produced\n",
    "expected_dirs = [pathlib.Path('../../data/raw'), pathlib.Path('../../data/processed'), pathlib.Path('../../data/interim')]\n",
    "for d in expected_dirs:\n",
    "    if d.is_dir():\n",
    "        files = list(d.glob('*.parquet'))[:10]\n",
    "        print(f\"Directory: {d} (showing up to 10 parquet files)\")\n",
    "        for f in files:\n",
    "            print('  -', f.name)\n",
    "    else:\n",
    "        print(f\"(Not found) {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8154060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Run Enhanced Modeling Script (replaces previous full pipeline modeling step)\n",
    "import subprocess, time, json, pathlib, os, sys\n",
    "\n",
    "start = time.time()\n",
    "print(\"Running enhanced modeling script: run_enhanced_modeling.py\")\n",
    "result = subprocess.run([sys.executable, 'run_enhanced_modeling.py'], capture_output=True, text=True)\n",
    "elapsed = time.time() - start\n",
    "print(f\"Enhanced modeling completed in {elapsed:.2f}s with return code {result.returncode}\\n\")\n",
    "print(\"--- STDOUT (first 3000 chars) ---\")\n",
    "print(result.stdout[:3000])\n",
    "print(\"--- STDERR (first 1000 chars) ---\")\n",
    "print(result.stderr[:1000])\n",
    "\n",
    "# Inspect enhanced modeling artifacts\n",
    "interim_dir = pathlib.Path('data/interim')\n",
    "models_dir = pathlib.Path('models')\n",
    "\n",
    "if interim_dir.is_dir():\n",
    "    enhanced_json = list(interim_dir.glob('enhanced_modeling_results.json'))\n",
    "    if enhanced_json:\n",
    "        try:\n",
    "            with open(enhanced_json[0], 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            print(f\"\\nLoaded enhanced modeling results summary: {enhanced_json[0]}\")\n",
    "            keys_preview = {k: data[k] for k in list(data.keys())[:8]}\n",
    "            print(json.dumps(keys_preview, indent=2)[:1500])\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read enhanced results: {e}\n",
    "\")\n",
    "    else:\n",
    "        print(\"No enhanced_modeling_results.json found yet.\")\n",
    "\n",
    "if models_dir.is_dir():\n",
    "    print(\"\\nModel artifacts (showing up to 15):\")\n",
    "    for p in list(models_dir.glob('*.json'))[:10] + list(models_dir.glob('*.pkl'))[:5]:\n",
    "        print('  -', p.name)\n",
    "else:\n",
    "    print(\"Models directory not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e515ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Run Term Decay Analysis Script\n",
    "import subprocess, time, pathlib\n",
    "\n",
    "start = time.time()\n",
    "print(\"Running term decay analysis script: term_decay_analysis.py\")\n",
    "result = subprocess.run([sys.executable, 'term_decay_analysis.py'], capture_output=True, text=True)\n",
    "elapsed = time.time() - start\n",
    "print(f\"Term decay analysis completed in {elapsed:.2f}s with return code {result.returncode}\\n\")\n",
    "print(\"--- STDOUT (first 3000 chars) ---\")\n",
    "print(result.stdout[:3000])\n",
    "print(\"--- STDERR (first 800 chars) ---\")\n",
    "print(result.stderr[:800])\n",
    "\n",
    "# Attempt to display any generated decay related JSON/CSV/PNG artifacts\n",
    "possible_dirs = [pathlib.Path('../../data/interim'), pathlib.Path('../../data/processed'), pathlib.Path('../../outputs'), pathlib.Path('../../reports')]\n",
    "for d in possible_dirs:\n",
    "    if d.is_dir():\n",
    "        decay_files = [p for p in d.rglob('*') if p.suffix.lower() in {'.json','.csv','.png'} and 'decay' in p.name.lower()][:10]\n",
    "        if decay_files:\n",
    "            print(f\"Found decay artifacts in {d}:\")\n",
    "            for f in decay_files:\n",
    "                print('  -', f.relative_to(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4138572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Capture and Display Key Outputs\n",
    "import pathlib, json, pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "root_candidates = [pathlib.Path('../../data'), pathlib.Path('../../models'), pathlib.Path('../../outputs')]\n",
    "for root in root_candidates:\n",
    "    if root.is_dir():\n",
    "        print(f\"\\nScanning: {root}\")\n",
    "        for p in list(root.rglob('*'))[:40]:\n",
    "            if p.is_file():\n",
    "                print('-', p.relative_to(root))\n",
    "\n",
    "# Attempt to load a metrics or summary JSON if present\n",
    "candidate_jsons = []\n",
    "for root in root_candidates:\n",
    "    if root.is_dir():\n",
    "        candidate_jsons.extend([p for p in root.rglob('*.json') if 'performance' in p.name.lower() or 'summary' in p.name.lower() or 'decay' in p.name.lower()])\n",
    "\n",
    "loaded = False\n",
    "for cj in candidate_jsons[:5]:\n",
    "    try:\n",
    "        with open(cj,'r',encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"\\nLoaded JSON summary: {cj}\")\n",
    "        if isinstance(data, dict):\n",
    "            print(json.dumps({k: data[k] for k in list(data)[:15]}, indent=2)[:1500])\n",
    "        loaded = True\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {cj}: {e}\")\n",
    "\n",
    "# Display head of a processed parquet if exists\n",
    "processed_parquets = list(pathlib.Path('../../data/processed').glob('*.parquet'))[:3]\n",
    "for pp in processed_parquets:\n",
    "    try:\n",
    "        df = pd.read_parquet(pp)\n",
    "        print(f\"\\nParquet sample: {pp}\")\n",
    "        display(df.head(3))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read {pp}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
