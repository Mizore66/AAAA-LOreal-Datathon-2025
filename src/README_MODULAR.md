# Modular Data Processing Pipeline

## ğŸ¯ Overview

This modular version replaces the original 1000+ line `data_processing_optimized.py` with a clean, maintainable architecture. The pipeline is broken down into 6 focused modules plus a clean 200-line driver file.

## ğŸ“ Module Structure

```
src/
â”œâ”€â”€ data_processing_modular.py     # Main driver (200 lines)
â”œâ”€â”€ test_modular_imports.py        # Import verification
â””â”€â”€ modules/
    â”œâ”€â”€ __init__.py                # Module initialization
    â”œâ”€â”€ config.py                  # Configuration & constants
    â”œâ”€â”€ text_processing.py         # Text cleaning & feature extraction
    â”œâ”€â”€ data_loading.py            # Dataset loading & preprocessing
    â”œâ”€â”€ aggregation.py             # Time-series aggregation
    â”œâ”€â”€ parallel_processing.py     # Concurrent processing utilities
    â””â”€â”€ reporting.py               # Enhanced report generation
```

## ğŸš€ Key Improvements

### âœ… Modular Architecture
- **6 focused modules** instead of one monolithic file
- **Single responsibility** for each module
- **Clean separation** of concerns
- **Easy to maintain** and extend

### âœ… Enhanced Progress Tracking
- **tqdm progress bars** for all timeframe processing
- **Emoji logging** for better visual feedback
- **Detailed progress** for each phase
- **Real-time status** updates

### âœ… Intelligent File Management
- **Skip existing files** to avoid reprocessing
- **Cache loaded data** for efficiency
- **Smart file detection** for parquet files
- **Memory optimization** through caching

### âœ… Enhanced Reporting
- **JSON reports** matching your provided format
- **Comprehensive analysis** with detailed metrics
- **Category breakdowns** and top performer analysis
- **Structured data** for easy programmatic access
- **Overlap analysis** between timeframes and sources

### 7. **Comprehensive Reporting**
- Multi-timeframe analysis
- Performance monitoring
- Clean markdown output

## Usage

### Run the Complete Pipeline
```bash
cd src
python data_processing_modular.py
```

### Import Individual Modules
```python
from modules import config, text_processing, aggregation
from modules.data_loading import load_samples_optimized
from modules.parallel_processing import parallel_process_sources
```

### Customize Configuration
```python
from modules.config import CONFIG, PERFORMANCE_MODE

# Change performance mode
# Options: "OPTIMIZED", "BALANCED", "THOROUGH"
CONFIG = PERF_CONFIGS["OPTIMIZED"]
```

## Configuration Options

### Performance Modes

- **OPTIMIZED**: Fast processing with aggressive sampling
- **BALANCED**: Good performance with reasonable accuracy (default)
- **THOROUGH**: Complete processing with full datasets

### Key Settings

- `max_features_emerging`: Limit emerging terms for performance
- `sample_rows_per_source`: Sample large datasets
- `enable_parallel`: Use concurrent processing
- `enable_chunking`: Split large datasets into chunks
- `chunk_size`: Size of processing chunks

## Output Files

The pipeline generates structured JSON reports matching your provided format:

```
data/
â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ features_hashtags_*.parquet      # Hashtag features by timeframe
â”‚   â”œâ”€â”€ features_keywords_*.parquet      # Keyword features by timeframe
â”‚   â”œâ”€â”€ features_emerging_terms_*.parquet # Emerging trends by timeframe
â”‚   â”œâ”€â”€ features_audio_*.parquet          # Audio features (6h only)
â”‚   â””â”€â”€ dataset_description.parquet       # Dataset metadata
â””â”€â”€ interim/
    â”œâ”€â”€ phase2_enhanced_features_comprehensive.json    # JSON Phase 2 analysis
    â”œâ”€â”€ phase3_emerging_trends_comprehensive.json      # JSON emerging trends report
    â””â”€â”€ performance_report.json                        # JSON performance metrics
```

## Benefits vs Original

1. **ğŸ”§ Maintainability**: Easy to modify individual components
2. **ğŸ§ª Testability**: Each module can be tested independently  
3. **ğŸ“ˆ Scalability**: Clear separation allows for easy extensions
4. **ğŸ” Readability**: Much smaller, focused files
5. **ğŸ”„ Reusability**: Modules can be used in other projects
6. **ğŸ› Debugging**: Easier to isolate and fix issues
7. **ğŸ‘¥ Collaboration**: Multiple developers can work on different modules

## Migration from Original

The modular pipeline is fully compatible with the original:
- Same input/output formats
- Same configuration options
- Same performance characteristics
- All optimizations preserved

Simply replace calls to `data_processing_optimized.py` with `data_processing_modular.py`.
